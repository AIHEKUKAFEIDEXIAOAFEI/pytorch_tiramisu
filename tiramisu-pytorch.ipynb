{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.autograd as autograd\n",
    "from pathlib import Path\n",
    "\n",
    "import imp\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import utils.training as train_utils\n",
    "\n",
    "\n",
    "from models import tiramisu\n",
    "from datasets import camvid\n",
    "from datasets import joint_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='data/'\n",
    "RESULTS_PATH='results/'\n",
    "WEIGHTS_PATH='models/'\n",
    "CAMVID_PATH=DATA_PATH+'CamVid/'\n",
    "EXPERIMENT='tiramisu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CamVid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TrainingSet = 367 frames\n",
    "* ValidationSet = 101 frames\n",
    "* TestSet = 233 frames\n",
    "* Images of resolution 360x480\n",
    "* Images \"Cropped\" to 224x224 for training\n",
    "* Full Resolution images used for finetuning\n",
    "* NumberOfClasses = 11 + background class\n",
    "* BatchSize = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seed = 0\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir = os.path.join(CAMVID_PATH, 'train')\n",
    "valdir = os.path.join(CAMVID_PATH, 'val')\n",
    "testdir = os.path.join(CAMVID_PATH, 'test')\n",
    "\n",
    "normalize = transforms.Normalize(mean=camvid.mean, std=camvid.std)\n",
    "train_joint_transformer = transforms.Compose([\n",
    "    #joint_transforms.JointRandomCrop(224),\n",
    "    joint_transforms.JointRandomHorizontalFlip()\n",
    "    ])\n",
    "train_dset = camvid.CamVid(CAMVID_PATH, 'train',\n",
    "      joint_transform=train_joint_transformer,\n",
    "      transform=transforms.Compose([\n",
    "          transforms.ToTensor(),\n",
    "          normalize,\n",
    "    ]))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dset = camvid.CamVid(\n",
    "    CAMVID_PATH, 'val', joint_transform=None,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dset = camvid.CamVid(\n",
    "    CAMVID_PATH, 'test', joint_transform=None,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_loader.dataset.classes)\n",
    "# print(train_loader.dataset.class_weight)\n",
    "# print(train_loader.dataset.imgs[:3])\n",
    "# print(train_loader.dataset.mean)\n",
    "# print(train_loader.dataset.std)\n",
    "print(\"TrainImages: %d\" %len(train_loader.dataset.imgs))\n",
    "print(\"ValImages: %d\" %len(val_loader.dataset.imgs))\n",
    "print(\"TestImages: %d\" %len(test_loader.dataset.imgs))\n",
    "print(\"NumClasses: %d\" % len(train_loader.dataset.classes))\n",
    "\n",
    "example_inputs, example_targets = next(iter(train_loader))\n",
    "print(\"InputsBatchSize: \", example_inputs.size())\n",
    "print(\"TargetsBatchSize: \", example_targets.size())\n",
    "\n",
    "#Inputs are tensors of normalized pixel values\n",
    "print (\"\\nInput (size, max, min) ---\")\n",
    "i = example_inputs[0]\n",
    "print (i.size())\n",
    "print(i.max())\n",
    "print(i.min())\n",
    "\n",
    "\n",
    "#Targets are tensors of class labels from 0-11 (0 means background)\n",
    "print (\"Target (size, max, min) ---\")\n",
    "t = example_targets[0]\n",
    "print(t.size())\n",
    "print(t.max())\n",
    "print(t.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "* https://discuss.pytorch.org/t/convert-pixel-wise-class-tensor-to-image-segmentation/1268\n",
    "* http://www.colorspire.com/rgb-color-wheel/ (verify RGB colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sky = [128,128,128]\n",
    "Building = [128,0,0]\n",
    "Pole = [192,192,128]\n",
    "Road = [128,64,128]\n",
    "Pavement = [60,40,222]\n",
    "Tree = [128,128,0]\n",
    "SignSymbol = [192,128,128]\n",
    "Fence = [64,64,128]\n",
    "Car = [64,0,128]\n",
    "Pedestrian = [64,64,0]\n",
    "Bicyclist = [0,128,192]\n",
    "Unlabelled = [0,0,0]\n",
    "\n",
    "label_colours = np.array([Sky, Building, Pole, Road, Pavement,\n",
    "      Tree, SignSymbol, Fence, Car, Pedestrian, Bicyclist, Unlabelled])\n",
    "\n",
    "def view_annotated(tensor, plot=True):\n",
    "    temp = tensor.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0,11):\n",
    "        r[temp==l]=label_colours[l,0]\n",
    "        g[temp==l]=label_colours[l,1]\n",
    "        b[temp==l]=label_colours[l,2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:,:,0] = (r/255.0)#[:,:,0]\n",
    "    rgb[:,:,1] = (g/255.0)#[:,:,1]\n",
    "    rgb[:,:,2] = (b/255.0)#[:,:,2]\n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return rgb\n",
    "\n",
    "def decode_image(tensor):\n",
    "    inp = tensor.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array(camvid.mean)\n",
    "    std = np.array(camvid.std)\n",
    "    inp = std * inp + mean\n",
    "    return inp\n",
    "\n",
    "def view_image(tensor):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = decode_image(tensor)\n",
    "    plt.imshow(inp)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, targets = next(iter(train_loader))\n",
    "#inputs, targets = next(iter(val_loader))\n",
    "#inputs, targets = next(iter(test_loader))\n",
    "\n",
    "# Plot Single Image\n",
    "view_image(inputs[0])\n",
    "\n",
    "# Plot Target Image\n",
    "view_annotated(targets[0])\n",
    "\n",
    "# Plot Grid of images\n",
    "out = torchvision.utils.make_grid(inputs, nrow=3)\n",
    "view_image(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visdom web browser\n",
    "\n",
    "* https://github.com/facebookresearch/visdom\n",
    "* https://github.com/facebookresearch/visdom/blob/master/example/demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visdom_enabled = True\n",
    "import visdom\n",
    "viz = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(train_loader))\n",
    "img_chart = viz.image(\n",
    "    np.random.rand(3,360,480),\n",
    "    opts=dict(title=\"Image\", caption='Silly random'),\n",
    ")\n",
    "viz_plot_img(img_chart, inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**\n",
    "\n",
    "* WeightInitialization = HeUniform\n",
    "* Optimizer = RMSProp\n",
    "* LR = .001 with exponential decay of 0.995 after each epoch\n",
    "* Data Augmentation = Random Crops, Vertical Flips\n",
    "* ValidationSet with early stopping based on IoU or MeanAccuracy with patience of 100 (50 during finetuning)\n",
    "* WeightDecay = .0001\n",
    "* Finetune with full-size images, LR = .0001\n",
    "* Dropout = 0.2\n",
    "* BatchNorm \"we use current batch stats at training, validation, and test time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(output_batch):\n",
    "    # Variables(Tensors) of size (bs,12,224,224)\n",
    "    bs,c,h,w = output_batch.size()\n",
    "    tensor = output_batch.data\n",
    "    # Argmax along channel axis (softmax probabilities)\n",
    "    values, indices = tensor.cpu().max(1)\n",
    "    indices = indices.view(bs,h,w)\n",
    "    return indices\n",
    "\n",
    "def error(preds, targets):\n",
    "    assert preds.size() == targets.size()\n",
    "    bs,h,w = preds.size()\n",
    "    n_pixels = bs*h*w\n",
    "    incorrect = preds.ne(targets).cpu().sum()\n",
    "    err = 100.*incorrect/n_pixels\n",
    "    return round(err,5)\n",
    "        \n",
    "def train(model, trn_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    trn_loss = 0\n",
    "    trn_error = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trn_loader):\n",
    "        inputs, targets = Variable(inputs.cuda()), Variable(targets.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trn_loss += loss.data[0]\n",
    "        pred = get_predictions(output)\n",
    "        trn_error += error(pred, targets.data.cpu())\n",
    "    trn_loss /= len(trn_loader) #n_batches\n",
    "    trn_error /= len(trn_loader)\n",
    "    return trn_loss, trn_error\n",
    "\n",
    "def test(model, test_loader, criterion, epoch=1):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_error = 0\n",
    "    for data, target in test_loader:\n",
    "        data = Variable(data.cuda(), volatile=True)\n",
    "        target = Variable(target.cuda())\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        pred = get_predictions(output)\n",
    "        test_error += error(pred, target.data.cpu())\n",
    "    test_loss /= len(test_loader) #n_batches\n",
    "    test_error /= len(test_loader)\n",
    "    return test_loss, test_error\n",
    "\n",
    "def adjust_learning_rate(lr, decay, optimizer, cur_epoch, n_epochs):\n",
    "    \"\"\"Sets the learning rate to the initially \n",
    "        configured `lr` decayed by `decay` every `n_epochs`\"\"\"\n",
    "    new_lr = lr * (decay ** (cur_epoch // n_epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        #kaiming is first name of author whose last name is 'He' lol\n",
    "        init.kaiming_uniform(m.weight) \n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "* https://github.com/SimJeg/FC-DenseNet/blob/master/config/FC-DenseNet103.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 12 #11 + background\n",
    "seed = 0\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_DECAY = 0.995 # Applied each epoch \"exponential decay\"\n",
    "DECAY_LR_EVERY_N_EPOCHS = 1\n",
    "WEIGHT_DECAY = 0.0001\n",
    "N_EPOCHS = 1000\n",
    "MAX_PATIENCE = 50\n",
    "\n",
    "train_file = RESULTS_PATH+EXPERIMENT+'-train.csv'\n",
    "test_file = RESULTS_PATH+EXPERIMENT+'-test.csv'\n",
    "existing_weights_fpath=WEIGHTS_PATH+'latest.pth'\n",
    "existing_optimizer_fpath=WEIGHTS_PATH+'latest-optim.pth'\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCDenseNet103(n_classes=12).cuda()\n",
    "print('  + Number of params: {}'.format(\n",
    "    sum([p.data.nelement() for p in model.parameters()])))\n",
    "model.apply(weights_init)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.NLLLoss2d(weight=camvid.class_weight.cuda()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment('tiramisu13', '/home/bfortuner/workplace/pytorch_tiramisu/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2\n",
    "START_EPOCH = exp.epoch\n",
    "END_EPOCH = START_EPOCH+N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(START_EPOCH, END_EPOCH):\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    ### Train ###\n",
    "    trn_loss, trn_err = train(model, train_loader, optimizer, criterion, epoch)\n",
    "    print('Epoch {:d}: Train - Loss: {:.4f}\\tErr: {:.4f}'.format(epoch, trn_loss, trn_err))    \n",
    "    time_elapsed = time.time() - since  \n",
    "    print('Train Time {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    \n",
    "    ### Test ###\n",
    "    val_loss, val_err = test(model, val_loader, criterion, epoch)    \n",
    "    print('Val - Loss: {:.4f}, Error: {:.4f}'.format(val_loss, val_err))\n",
    "    time_elapsed = time.time() - since  \n",
    "    print('Total Time {:.0f}m {:.0f}s\\n'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    \n",
    "    ### Save Metrics ###\n",
    "    exp.save_history('train', trn_loss, trn_err)\n",
    "    exp.save_history('val', val_loss, val_err)\n",
    "    \n",
    "    \n",
    "    ### Checkpoint ###    \n",
    "    exp.save_weights(model, trn_loss, val_loss, trn_err, val_err)\n",
    "    exp.save_optimizer(optimizer, val_loss)\n",
    "\n",
    "    \n",
    "    ### Plot Online ###\n",
    "    exp.update_viz_loss_plot()\n",
    "    exp.update_viz_error_plot()\n",
    "    exp.update_viz_summary_plot()\n",
    "    \n",
    "    \n",
    "    ## Early Stopping ##\n",
    "    if (epoch - exp.best_val_loss_epoch) > MAX_PATIENCE:\n",
    "        print((\"Early stopping at epoch %d since no \" \n",
    "               + \"better loss found since epoch %.3\") \n",
    "               % (epoch, exp.best_val_loss))\n",
    "        break\n",
    "\n",
    "\n",
    "    ### Adjust Lr ###\n",
    "    adjust_learning_rate(LEARNING_RATE, LR_DECAY, optimizer, \n",
    "                         epoch, DECAY_LR_EVERY_N_EPOCHS)\n",
    "    \n",
    "    exp.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment('tiramisu12', '/home/bfortuner/workplace/pytorch_tiramisu/')\n",
    "exp.resume(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss2d(weight=camvid.class_weight.cuda()).cuda()\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_loader, n_batches=1):\n",
    "    input_loader.batch_size = 1\n",
    "    #Takes input_loader and returns array of prediction tensors\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    for input, target in input_loader:\n",
    "        data, label = Variable(input.cuda(), volatile=True), Variable(target.cuda())\n",
    "        output = model(data)\n",
    "        pred = get_predictions(output)\n",
    "        predictions.append([input,target,pred])\n",
    "    return predictions\n",
    "\n",
    "#predictions = predict_all(model, test_loader, 1)\n",
    "# for out in predictions[:1]:\n",
    "#     view_image(out[0][0])\n",
    "#     view_annotated(out[1][0])\n",
    "#     view_annotated(out[2][0])\n",
    "\n",
    "def view_sample_predictions(n):\n",
    "    #torch.cuda.manual_seed(random.randint(0,10**7))\n",
    "    test_loader = torch.utils.data.DataLoader(test_dset, batch_size=n, shuffle=True)\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    data, label = Variable(inputs.cuda(), volatile=True), Variable(targets.cuda())\n",
    "    output = model(data)\n",
    "    pred = get_predictions(output)\n",
    "    batch_size = inputs.size(0)\n",
    "    for i in range(batch_size):\n",
    "        view_image(inputs[i])\n",
    "        view_annotated(targets[i])\n",
    "        view_annotated(pred[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "view_sample_predictions(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "* https://github.com/mattmacy/vnet.pytorch/blob/master/train.py\n",
    "* https://github.com/SimJeg/FC-DenseNet/blob/master/FC-DenseNet.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
