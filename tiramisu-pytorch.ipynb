{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import imp\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import utils.training as train_utils; imp.reload(train_utils)\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH='data/'\n",
    "RESULTS_PATH='results/'\n",
    "WEIGHTS_PATH='models/'\n",
    "PROJECT_NAME='tiramisu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**FirstConvLayer**\n",
    "\n",
    "* 3x3 Conv2D (pad=, stride=, in_chans=3, out_chans=48)\n",
    "\n",
    "**DenseLayer**\n",
    "\n",
    "* BatchNorm\n",
    "* ReLU\n",
    "* 3x3 Conv2d (pad=, stride=, in_chans=, out_chans=) - \"no resolution loss\" - padding included\n",
    "* Dropout (.2)\n",
    "\n",
    "**DenseBlock**\n",
    "\n",
    "* Input = FirstConvLayer, TransitionDown, or TransitionUp\n",
    "* Loop to create L DenseLayers (L=n_layers)\n",
    "* On TransitionDown we Concat(Input, FinalDenseLayerActivation)\n",
    "* On TransitionUp we do not Concat with input, instead pass FinalDenseLayerActivation to TransitionUp block\n",
    "\n",
    "**TransitionDown**\n",
    "\n",
    "* BatchNorm\n",
    "* ReLU\n",
    "* 1x1 Conv2D (pad=, stride=, in_chans=, out_chans=)\n",
    "* Dropout (0.2)\n",
    "* 2x2 MaxPooling\n",
    "\n",
    "**Bottleneck**\n",
    "\n",
    "* DenseBlock (15 layers)\n",
    "\n",
    "**TransitionUp**\n",
    "\n",
    "* 3x3 Transposed Convolution (pad=, stride=2, in_chans=, out_chans=)\n",
    "* Concat(PreviousDenseBlock, SkipConnection) - from cooresponding DenseBlock on transition down\n",
    "\n",
    "**FinalBlock**\n",
    "\n",
    "* 1x1 Conv2d (pad=, stride=, in_chans=256, out_chans=n_classes)\n",
    "* Softmax\n",
    "\n",
    "**FCDenseNet103 Architecture**\n",
    "\n",
    "* input (in_chans=3 for RGB)\n",
    "* 3x3 ConvLayer (out_chans=48)\n",
    "* DB (4 layers) + TD\n",
    "* DB (5 layers) + TD\n",
    "* DB (7 layers) + TD\n",
    "* DB (10 layers) + TD\n",
    "* DB (12 layers) + TD\n",
    "* Bottleneck (15 layers)\n",
    "* TU + DB (12 layers)\n",
    "* TU + DB (10 layers)\n",
    "* TU + DB (7 layers)\n",
    "* TU + DB (5 layers)\n",
    "* TU + DB (4 layers)\n",
    "* 1x1 ConvLayer (out_chans=n_classes) n_classes=11 for CamVid\n",
    "* Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, nChannels, nLayers):\n",
    "        super(DenseBlock, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class TransitionDown(nn.Module):\n",
    "    def __init__(self, nChannels, nLayers):\n",
    "        super(TransitionDown, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class TransitionUp(nn.Module):\n",
    "    def __init__(self, nChannels, nLayers):\n",
    "        super(TransitionUp, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FCDenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCDenseNet, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "CIFAR10_PATH=DATA_PATH+'cifar10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.manual_seed(1)\n",
    "\n",
    "normMean = [0.49139968, 0.48215827, 0.44653124]\n",
    "normStd = [0.24703233, 0.24348505, 0.26158768]\n",
    "normTransform = transforms.Normalize(normMean, normStd)\n",
    "\n",
    "trainTransform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normTransform\n",
    "])\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normTransform\n",
    "])\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "trainLoader = DataLoader(\n",
    "    dset.CIFAR10(root=CIFAR10_PATH, train=True, download=True,\n",
    "                 transform=trainTransform),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "testLoader = DataLoader(\n",
    "    dset.CIFAR10(root=CIFAR10_PATH, train=False, download=True,\n",
    "                 transform=testTransform),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Hyperparameters**\n",
    "\n",
    "* WeightInitialization = HeUniform\n",
    "* Optimizer = RMSProp\n",
    "* LR = .001 with exponential decay of 0.995 after each epoch\n",
    "* Data Augmentation = Random Crops, Vertical Flips\n",
    "* ValidationSet with early stopping based on IoU or MeanAccuracy with patience of 100 (50 during finetuning)\n",
    "* WeightDecay = .0001\n",
    "* Finetune with full-size images, LR = .0001\n",
    "* Dropout = 0.2\n",
    "* BatchNorm \"we use current batch stats at training, validation, and test time\"\n",
    "\n",
    "**CamVid**\n",
    "\n",
    "* TrainingSet = 367 frames\n",
    "* ValidationSet = 101 frames\n",
    "* TestSet = 233 frames\n",
    "* Images of resolution 360x480\n",
    "* Images \"Cropped\" to 224x224 for training --- center crop?\n",
    "* FullRes images used for finetuning\n",
    "* NumberOfClasses = 11 (output)\n",
    "* BatchSize = 3\n",
    "\n",
    "**FCDenseNet103**\n",
    "\n",
    "* GrowthRate = 16 (k, number of filters to each denselayer adds to the ever-growing concatenated output)\n",
    "* No pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  + Number of params: 1059298\n",
      "Training new model from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train - Loss: 1.370707\tError: 43.750000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [01:12<00:00, 72.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Loss: 1.3404, Error: 4590/10000 (46%)\n",
      "Time 1m 12s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "existing_weights_fpath=None\n",
    "nEpochs=1\n",
    "\n",
    "net = DenseNet(growthRate=12, depth=40, reduction=1.0, \n",
    "                   bottleneck=False, nClasses=10)\n",
    "net = net.cuda()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-1,\n",
    "                momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "print('  + Number of params: {}'.format(\n",
    "    sum([p.data.nelement() for p in net.parameters()])))\n",
    "\n",
    "if existing_weights_fpath:\n",
    "    startEpoch = train_utils.load_weights(net, existing_weights_fpath)\n",
    "    endEpoch = startEpoch + nEpochs\n",
    "    print ('Resume training at epoch: {}'.format(startEpoch))\n",
    "    if os.path.exists(RESULTS_PATH+'train.csv'): #assume test.csv exists\n",
    "        append_write = 'a' # append if already exists\n",
    "    else:\n",
    "        append_write = 'w' # make a new file if not\n",
    "    trainF = open(os.path.join(RESULTS_PATH, 'train.csv'), append_write)\n",
    "    testF = open(os.path.join(RESULTS_PATH, 'test.csv'), append_write)\n",
    "else:\n",
    "    print (\"Training new model from scratch\")\n",
    "    startEpoch = 1\n",
    "    endEpoch = nEpochs\n",
    "    trainF = open(os.path.join(RESULTS_PATH, 'train.csv'), 'w')\n",
    "    testF = open(os.path.join(RESULTS_PATH, 'test.csv'), 'w')\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(startEpoch, endEpoch+1)):\n",
    "    since = time.time()\n",
    "    train_utils.adjust_opt(\"sgd\", optimizer, epoch)\n",
    "    train_utils.train(epoch, net, trainLoader, optimizer, trainF)\n",
    "    train_utils.test(epoch, net, testLoader, optimizer, testF)\n",
    "    time_elapsed = time.time() - since  \n",
    "    print('Time {:.0f}m {:.0f}s\\n'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if epoch != 1:\n",
    "        os.system('./utils/plot.py {} &'.format(RESULTS_PATH))\n",
    "\n",
    "trainF.close()\n",
    "testF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
