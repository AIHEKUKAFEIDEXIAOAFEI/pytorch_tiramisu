{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import imp\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import utils.training as train_utils; imp.reload(train_utils)\n",
    "#import utils.plot as plot_utils; imp.reload(plot_utils)\n",
    "\n",
    "import camvid_dataset as camvid\n",
    "import joint_transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH='data/'\n",
    "RESULTS_PATH='results/'\n",
    "WEIGHTS_PATH='models/'\n",
    "CAMVID_PATH=DATA_PATH+'CamVid/'\n",
    "EXPERIMENT='tiramisu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**FirstConvLayer**\n",
    "\n",
    "* 3x3 Conv2D (pad=, stride=, in_chans=3, out_chans=48)\n",
    "\n",
    "**DenseLayer**\n",
    "\n",
    "* BatchNorm\n",
    "* ReLU\n",
    "* 3x3 Conv2d (pad=, stride=, in_chans=, out_chans=) - \"no resolution loss\" - padding included\n",
    "* Dropout (.2)\n",
    "\n",
    "**DenseBlock**\n",
    "\n",
    "* Input = FirstConvLayer, TransitionDown, or TransitionUp\n",
    "* Loop to create L DenseLayers (L=n_layers)\n",
    "* On TransitionDown we Concat(Input, FinalDenseLayerActivation)\n",
    "* On TransitionUp we do not Concat with input, instead pass FinalDenseLayerActivation to TransitionUp block\n",
    "\n",
    "**TransitionDown**\n",
    "\n",
    "* BatchNorm\n",
    "* ReLU\n",
    "* 1x1 Conv2D (pad=, stride=, in_chans=, out_chans=)\n",
    "* Dropout (0.2)\n",
    "* 2x2 MaxPooling\n",
    "\n",
    "**Bottleneck**\n",
    "\n",
    "* DenseBlock (15 layers)\n",
    "\n",
    "**TransitionUp**\n",
    "\n",
    "* 3x3 Transposed Convolution (pad=, stride=2, in_chans=, out_chans=)\n",
    "* Concat(PreviousDenseBlock, SkipConnection) - from cooresponding DenseBlock on transition down\n",
    "\n",
    "**FinalBlock**\n",
    "\n",
    "* 1x1 Conv2d (pad=, stride=, in_chans=256, out_chans=n_classes)\n",
    "* Softmax\n",
    "\n",
    "**FCDenseNet103 Architecture**\n",
    "\n",
    "* input (in_chans=3 for RGB)\n",
    "* 3x3 ConvLayer (out_chans=48)\n",
    "* DB (4 layers) + TD\n",
    "* DB (5 layers) + TD\n",
    "* DB (7 layers) + TD\n",
    "* DB (10 layers) + TD\n",
    "* DB (12 layers) + TD\n",
    "* Bottleneck (15 layers)\n",
    "* TU + DB (12 layers)\n",
    "* TU + DB (10 layers)\n",
    "* TU + DB (7 layers)\n",
    "* TU + DB (5 layers)\n",
    "* TU + DB (4 layers)\n",
    "* 1x1 ConvLayer (out_chans=n_classes) n_classes=11 for CamVid\n",
    "* Softmax\n",
    "\n",
    "**FCDenseNet67**\n",
    "\n",
    "* GrowthRate (k) = 16\n",
    "* 5 layers per dense block\n",
    "* 1 Conv Layer\n",
    "* 5 DenseBlocks Downsample (25 layers)\n",
    "* 5 TransitionDown\n",
    "* 5 Bottleneck layers\n",
    "* 5 Dense Blocks Upsample (25 layers)\n",
    "* 5 TransitionUp\n",
    "* 1 Conv Layer\n",
    "* 1 Softmax layer (doesn't count)\n",
    "67 Total layers\n",
    "\n",
    "**360x480 Input Path**\n",
    "\n",
    "Image dimensions that are evenly divisible are nice. The 224x224 input work nicely w/out cropping.\n",
    "* skipsize torch.Size([1, 128, 360, 480])\n",
    "* skipsize torch.Size([1, 208, 180, 240])\n",
    "* skipsize torch.Size([1, 288, 90, 120])\n",
    "* skipsize torch.Size([1, 368, 45, 60])\n",
    "* skipsize torch.Size([1, 448, 22, 30])    <------- we lose 1 pixel here 22.5 to 22 b/c of rounding\n",
    "* bnecksize torch.Size([1, 80, 11, 15])\n",
    "* insize torch.Size([1, 80, 11, 15])\n",
    "* outsize torch.Size([1, 80, 22, 30])   \n",
    "* insize torch.Size([1, 80, 22, 30])  <--------- we need to crop/pad to recover that lost pixel\n",
    "* outsize torch.Size([1, 80, 45, 60])\n",
    "* insize torch.Size([1, 80, 45, 60])\n",
    "* outsize torch.Size([1, 80, 90, 120])\n",
    "* insize torch.Size([1, 80, 90, 120])\n",
    "* outsize torch.Size([1, 80, 180, 240])\n",
    "* insize torch.Size([1, 80, 180, 240])\n",
    "* outsize torch.Size([1, 80, 360, 480])\n",
    "\n",
    "\n",
    "**224x224 Input Path**\n",
    "* skipsize torch.Size([3, 128, 224, 224])\n",
    "* skipsize torch.Size([3, 208, 112, 112])\n",
    "* skipsize torch.Size([3, 288, 56, 56])\n",
    "* skipsize torch.Size([3, 368, 28, 28])\n",
    "* skipsize torch.Size([3, 448, 14, 14])\n",
    "* bnecksize torch.Size([3, 80, 7, 7])\n",
    "* insize torch.Size([3, 80, 7, 7])\n",
    "* outsize torch.Size([3, 80, 14, 14])\n",
    "* insize torch.Size([3, 80, 14, 14])\n",
    "* outsize torch.Size([3, 80, 28, 28])\n",
    "* insize torch.Size([3, 80, 28, 28])\n",
    "* outsize torch.Size([3, 80, 56, 56])\n",
    "* insize torch.Size([3, 80, 56, 56])\n",
    "* outsize torch.Size([3, 80, 112, 112])\n",
    "* insize torch.Size([3, 80, 112, 112])\n",
    "* outsize torch.Size([3, 80, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def center_crop(layer, max_height, max_width):\n",
    "    #https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/merge.py#L162\n",
    "    #Author does a center crop which crops both inputs (skip and upsample) to size of minimum dimension on both w/h\n",
    "    batch_size, n_channels, layer_height, layer_width = layer.size()\n",
    "    xy1 = (layer_width - max_width) // 2\n",
    "    xy2 = (layer_height - max_height) // 2\n",
    "    return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        \n",
    "        #author's impl - lasange 'same' pads with half \n",
    "        # filter size (rounded down) on \"both\" sides\n",
    "        self.add_module('conv', nn.Conv2d(in_channels=in_channels, \n",
    "                out_channels=growth_rate, kernel_size=3, stride=1, \n",
    "                  padding=1, bias=True))\n",
    "        \n",
    "        self.add_module('drop', nn.Dropout2d(0.2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(DenseLayer, self).forward(x)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.layers = nn.ModuleList([DenseLayer(\n",
    "            in_channels + i*growth_rate, growth_rate)\n",
    "            for i in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            new_features = []\n",
    "            #we pass all previous activations into each dense layer normally\n",
    "            #But we only store each dense layer's output in the new_features array\n",
    "            for layer in self.layers:\n",
    "                out = layer(x)\n",
    "                x = torch.cat([x, out], 1)\n",
    "                new_features.append(out)\n",
    "            return torch.cat(new_features,1)\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                out = layer(x)\n",
    "                x = torch.cat([x, out], 1) # 1 = channel axis\n",
    "            return x \n",
    "    \n",
    "class TransitionDown(nn.Sequential):\n",
    "    def __init__(self, in_channels):\n",
    "        super(TransitionDown, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(in_channels=in_channels, \n",
    "              out_channels=in_channels, kernel_size=1, stride=1, \n",
    "                padding=0, bias=True))\n",
    "        self.add_module('drop', nn.Dropout2d(0.2))\n",
    "        self.add_module('maxpool', nn.MaxPool2d(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return super(TransitionDown, self).forward(x)\n",
    "    \n",
    "class TransitionUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransitionUp, self).__init__()\n",
    "        self.convTrans = nn.ConvTranspose2d(in_channels=in_channels, \n",
    "               out_channels=out_channels, kernel_size=3, stride=2, \n",
    "              padding=0, bias=True) #crop = 'valid' means padding=0. Padding has reverse effect for transpose conv (reduces output size)\n",
    "        #http://lasagne.readthedocs.io/en/latest/modules/layers/conv.html#lasagne.layers.TransposedConv2DLayer\n",
    "        #self.updample2d = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        out = self.convTrans(x)\n",
    "        out = center_crop(out, skip.size(2), skip.size(3))\n",
    "        out = torch.cat([out, skip], 1)\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate, n_layers):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.add_module('bottleneck', DenseBlock(in_channels, growth_rate, n_layers, upsample=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(Bottleneck, self).forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FCDenseNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, down_blocks=(5,5,5,5,5), \n",
    "                 up_blocks=(5,5,5,5,5), bottleneck_layers=5, \n",
    "                 growth_rate=16, out_chans_first_conv=48, n_classes=12):\n",
    "        super(FCDenseNet, self).__init__()\n",
    "        self.down_blocks = down_blocks\n",
    "        self.up_blocks = up_blocks\n",
    "        \n",
    "        cur_channels_count = 0\n",
    "        skip_connection_channel_counts = []\n",
    "        \n",
    "        \n",
    "        #####################\n",
    "        # First Convolution #\n",
    "        #####################\n",
    "\n",
    "        self.add_module('firstconv', nn.Conv2d(in_channels=in_channels, \n",
    "                  out_channels=out_chans_first_conv, kernel_size=3, \n",
    "                  stride=1, padding=1, bias=True))\n",
    "        cur_channels_count = out_chans_first_conv\n",
    "        \n",
    "        \n",
    "        \n",
    "        #####################\n",
    "        # Downsampling path #\n",
    "        #####################\n",
    "        \n",
    "        self.denseBlocksDown = nn.ModuleList([])\n",
    "        self.transDownBlocks = nn.ModuleList([])\n",
    "        for i in range(len(down_blocks)):\n",
    "            self.denseBlocksDown.append(\n",
    "                DenseBlock(cur_channels_count, growth_rate, down_blocks[i]))\n",
    "            cur_channels_count += (growth_rate*down_blocks[i])\n",
    "            skip_connection_channel_counts.insert(0,cur_channels_count)\n",
    "            self.transDownBlocks.append(TransitionDown(cur_channels_count))\n",
    "            \n",
    "            \n",
    "            \n",
    "        #####################\n",
    "        #     Bottleneck    #\n",
    "        #####################\n",
    "        \n",
    "        self.add_module('bottleneck',Bottleneck(cur_channels_count, \n",
    "                                     growth_rate, bottleneck_layers))\n",
    "        prev_block_channels = growth_rate*bottleneck_layers\n",
    "        cur_channels_count += prev_block_channels \n",
    "        \n",
    "        \n",
    "        \n",
    "        #######################\n",
    "        #   Upsampling path   #\n",
    "        #######################\n",
    "\n",
    "        self.transUpBlocks = nn.ModuleList([])\n",
    "        self.denseBlocksUp = nn.ModuleList([])\n",
    "        for i in range(len(up_blocks)-1):\n",
    "            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n",
    "            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n",
    "\n",
    "            self.denseBlocksUp.append(DenseBlock(\n",
    "                cur_channels_count, growth_rate, up_blocks[i], \n",
    "                    upsample=True))\n",
    "            prev_block_channels = growth_rate*up_blocks[i]\n",
    "            cur_channels_count += prev_block_channels\n",
    "\n",
    "            \n",
    "        #One final dense block\n",
    "        self.transUpBlocks.append(TransitionUp(\n",
    "            prev_block_channels, prev_block_channels))\n",
    "        cur_channels_count = prev_block_channels + skip_connection_channel_counts[-1]\n",
    "\n",
    "        self.denseBlocksUp.append(DenseBlock(\n",
    "            cur_channels_count, growth_rate, up_blocks[-1], \n",
    "                upsample=False))\n",
    "        cur_channels_count += growth_rate*up_blocks[-1]\n",
    "\n",
    "        \n",
    "        \n",
    "        #####################\n",
    "        #      Softmax      #\n",
    "        #####################\n",
    "\n",
    "        self.finalConv = nn.Conv2d(in_channels=cur_channels_count, \n",
    "               out_channels=n_classes, kernel_size=1, stride=1, \n",
    "                   padding=0, bias=True)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"INPUT\",x.size())\n",
    "        out = self.firstconv(x)\n",
    "        \n",
    "        skip_connections = []\n",
    "        for i in range(len(self.down_blocks)):\n",
    "            #print(\"DBD size\",out.size())\n",
    "            out = self.denseBlocksDown[i](out)\n",
    "            skip_connections.append(out)\n",
    "            out = self.transDownBlocks[i](out)\n",
    "            \n",
    "        out = self.bottleneck(out)\n",
    "        #print (\"bnecksize\",out.size())\n",
    "        for i in range(len(self.up_blocks)):\n",
    "            skip = skip_connections.pop()\n",
    "            #print(\"DOWN_SKIP_PRE_UPSAMPLE\",out.size(),skip.size())\n",
    "            out = self.transUpBlocks[i](out, skip)\n",
    "            #print(\"DOWN_SKIP_AFT_UPSAMPLE\",out.size(),skip.size())\n",
    "            out = self.denseBlocksUp[i](out)\n",
    "            \n",
    "        out = self.finalConv(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "def FCDenseNet57(n_classes):\n",
    "    return FCDenseNet(in_channels=3, down_blocks=(4, 4, 4, 4, 4), \n",
    "                 up_blocks=(4, 4, 4, 4, 4), bottleneck_layers=4, \n",
    "                 growth_rate=12, out_chans_first_conv=48, n_classes=n_classes)\n",
    "\n",
    "def FCDenseNet67(n_classes):\n",
    "    return FCDenseNet(in_channels=3, down_blocks=(5, 5, 5, 5, 5), \n",
    "                 up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5, \n",
    "                 growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)\n",
    "\n",
    "def FCDenseNet103(n_classes):\n",
    "    return FCDenseNet(in_channels=3, down_blocks=(4,5,7,10,12), \n",
    "                 up_blocks=(12,10,7,5,4), bottleneck_layers=15, \n",
    "                 growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(FCDenseNet103(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "* https://github.com/SimJeg/FC-DenseNet/blob/master/data_loader.py\n",
    "* https://github.com/pytorch/vision/pull/90\n",
    "* https://github.com/SimJeg/FC-DenseNet/issues/10\n",
    "\n",
    "\n",
    "**CamVid**\n",
    "\n",
    "* TrainingSet = 367 frames\n",
    "* ValidationSet = 101 frames\n",
    "* TestSet = 233 frames\n",
    "* Images of resolution 360x480\n",
    "* Images \"Cropped\" to 224x224 for training --- center crop?\n",
    "* FullRes images used for finetuning\n",
    "* NumberOfClasses = 11 (output)\n",
    "* BatchSize = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seed = 0\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "traindir = os.path.join(CAMVID_PATH, 'train')\n",
    "valdir = os.path.join(CAMVID_PATH, 'val')\n",
    "testdir = os.path.join(CAMVID_PATH, 'test')\n",
    "\n",
    "normalize = transforms.Normalize(mean=camvid.mean, std=camvid.std)\n",
    "train_joint_transformer = transforms.Compose([\n",
    "    #joint_transforms.JointRandomCrop(224),\n",
    "    joint_transforms.JointRandomHorizontalFlip()\n",
    "    ])\n",
    "train_dset = camvid.CamVid(CAMVID_PATH, 'train',\n",
    "      joint_transform=train_joint_transformer,\n",
    "      transform=transforms.Compose([\n",
    "          transforms.ToTensor(),\n",
    "          normalize,\n",
    "    ]))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dset = camvid.CamVid(\n",
    "    CAMVID_PATH, 'val', joint_transform=None,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dset = camvid.CamVid(\n",
    "    CAMVID_PATH, 'test', joint_transform=None,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(train_loader.dataset.classes)\n",
    "# print(train_loader.dataset.class_weight)\n",
    "# print(train_loader.dataset.imgs[:3])\n",
    "# print(train_loader.dataset.mean)\n",
    "# print(train_loader.dataset.std)\n",
    "print(\"TrainImages: %d\" %len(train_loader.dataset.imgs))\n",
    "print(\"ValImages: %d\" %len(val_loader.dataset.imgs))\n",
    "print(\"TestImages: %d\" %len(test_loader.dataset.imgs))\n",
    "print(\"NumClasses: %d\" % len(train_loader.dataset.classes))\n",
    "\n",
    "example_inputs, example_targets = next(iter(train_loader))\n",
    "print(\"InputsBatchSize: \", example_inputs.size())\n",
    "print(\"TargetsBatchSize: \", example_targets.size())\n",
    "\n",
    "#Inputs are tensors of normalized pixel values\n",
    "print (\"\\nInput (size, max, min) ---\")\n",
    "i = example_inputs[0]\n",
    "print (i.size())\n",
    "print(i.max())\n",
    "print(i.min())\n",
    "\n",
    "\n",
    "#Targets are tensors of class labels from 0-11 (0 means background)\n",
    "print (\"Target (size, max, min) ---\")\n",
    "t = example_targets[0]\n",
    "print(t.size())\n",
    "print(t.max())\n",
    "print(t.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Visualize\n",
    "\n",
    "* https://discuss.pytorch.org/t/convert-pixel-wise-class-tensor-to-image-segmentation/1268\n",
    "* http://www.colorspire.com/rgb-color-wheel/ (verify RGB colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Sky = [128,128,128]\n",
    "Building = [128,0,0]\n",
    "Pole = [192,192,128]\n",
    "Road = [128,64,128]\n",
    "Pavement = [60,40,222]\n",
    "Tree = [128,128,0]\n",
    "SignSymbol = [192,128,128]\n",
    "Fence = [64,64,128]\n",
    "Car = [64,0,128]\n",
    "Pedestrian = [64,64,0]\n",
    "Bicyclist = [0,128,192]\n",
    "Unlabelled = [0,0,0]\n",
    "\n",
    "label_colours = np.array([Sky, Building, Pole, Road, Pavement,\n",
    "      Tree, SignSymbol, Fence, Car, Pedestrian, Bicyclist, Unlabelled])\n",
    "\n",
    "def view_annotated(tensor, plot=True):\n",
    "    temp = tensor.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0,11):\n",
    "        r[temp==l]=label_colours[l,0]\n",
    "        g[temp==l]=label_colours[l,1]\n",
    "        b[temp==l]=label_colours[l,2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:,:,0] = (r/255.0)#[:,:,0]\n",
    "    rgb[:,:,1] = (g/255.0)#[:,:,1]\n",
    "    rgb[:,:,2] = (b/255.0)#[:,:,2]\n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return rgb\n",
    "\n",
    "def decode_image(tensor):\n",
    "    inp = tensor.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array(camvid.mean)\n",
    "    std = np.array(camvid.std)\n",
    "    inp = std * inp + mean\n",
    "    return inp\n",
    "\n",
    "def view_image(tensor):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = decode_image(tensor)\n",
    "    plt.imshow(inp)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, targets = next(iter(train_loader))\n",
    "#inputs, targets = next(iter(val_loader))\n",
    "#inputs, targets = next(iter(test_loader))\n",
    "\n",
    "# Plot Single Image\n",
    "view_image(inputs[0])\n",
    "\n",
    "# Plot Target Image\n",
    "view_annotated(targets[0])\n",
    "\n",
    "# Plot Grid of images\n",
    "out = torchvision.utils.make_grid(inputs, nrow=3)\n",
    "view_image(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Visdom web browser\n",
    "\n",
    "* https://github.com/facebookresearch/visdom\n",
    "* https://github.com/facebookresearch/visdom/blob/master/example/demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visdom_enabled = True\n",
    "import visdom\n",
    "viz = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def viz_plot_tst_trn(window, epoch, tst_val, trn_val, name='loss', env='main'):\n",
    "    if window is None:\n",
    "        return viz.line(\n",
    "            X=np.array([epoch]),\n",
    "            Y=np.array([[tst_val, trn_val]]),\n",
    "            opts=dict(\n",
    "                xlabel='epoch',\n",
    "                ylabel=name,\n",
    "                title=env+' '+name,\n",
    "                legend=['Validation', 'Train']\n",
    "            ),\n",
    "            env=env\n",
    "        )\n",
    "    return viz.line(\n",
    "        X=np.ones((1, 2)) * epoch,\n",
    "        Y=np.expand_dims([tst_val, trn_val],0),\n",
    "        win=window,\n",
    "        update='append',\n",
    "        env=env\n",
    "    )\n",
    "\n",
    "def viz_plot_img(window, tensor, env='main', title='Image'):\n",
    "    '''\n",
    "    This function draws an img on your Visdom web app. \n",
    "    It takes as input an `CxHxW` tensor `img`\n",
    "    The array values can be float in [0,1] or uint8 in [0, 255]'''\n",
    "    np_img = decode_image(tensor)\n",
    "    np_img = np.rollaxis(np_img, 2, 0)\n",
    "    viz.image(\n",
    "        np_img,\n",
    "        opts=dict(title=title, caption='Silly image'),\n",
    "        win=window,\n",
    "        env=env\n",
    "    )\n",
    "    \n",
    "def viz_plot_text(window, text, env='main'):\n",
    "    if window is None:\n",
    "        return viz.text(\n",
    "            text,\n",
    "            env=env\n",
    "        )\n",
    "    return viz.text(\n",
    "        text,\n",
    "        win=window,\n",
    "        env=env\n",
    "    )\n",
    "\n",
    "def viz_plot_summary(window, epoch, tst_loss, trn_loss,\n",
    "                       tst_err, trn_err, env='main'):\n",
    "    txt = (\"\"\"Epoch: %d\n",
    "        Train - Loss: %.3f Err: %.3f\n",
    "        Test - Loss: %.3f Err: %.3f\"\"\" % (epoch, \n",
    "        trn_loss, trn_err, tst_loss, tst_err))\n",
    "    return viz_plot_text(window, txt, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Should plot one chart and update it\n",
    "txt_chart = viz_plot_summary(None, 1, 2, 3, 4, 5)\n",
    "txt_chart = viz_plot_summary(txt_chart, 5, 2, 3, 4, 5)\n",
    "txt_chart = viz_plot_summary(txt_chart, 5, 3, 8, 7, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Should plot one chart and update it\n",
    "sum_chart = viz_plot_text(None, 'Hello, world3!')\n",
    "sum_chart = viz_plot_text(sum_chart, 'Hello, world4!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Should plot one chart and update it\n",
    "#window, epoch, tst_val, trn_val, name='loss', env='main'\n",
    "loss_chart = viz_plot_tst_trn(None, 9, 14, 27, 'loss')\n",
    "loss_chart = viz_plot_tst_trn(loss_chart, 10, 18, 30, 'loss')\n",
    "loss_chart = viz_plot_tst_trn(loss_chart, 11, 19, 32, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Should plot one chart and update it\n",
    "#window, epoch, tst_val, trn_val, name='loss', env='main'\n",
    "err_chart = viz_plot_tst_trn(None, 9, 14, 27, 'error')\n",
    "err_chart = viz_plot_tst_trn(err_chart, 10, 18, 30, 'error')\n",
    "err_chart = viz_plot_tst_trn(err_chart, 11, 19, 32, 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(train_loader))\n",
    "img_chart = viz.image(\n",
    "    np.random.rand(3,360,480),\n",
    "    opts=dict(title=\"Image\", caption='Silly random'),\n",
    ")\n",
    "viz_plot_img(img_chart, inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "* https://github.com/ycszen/pytorch-ss/blob/master/loss.py\n",
    "* http://pytorch.org/docs/nn.html?highlight=logsoftmax#nllloss2d\n",
    "* https://github.com/SimJeg/FC-DenseNet/blob/master/metrics.py\n",
    "* https://cs231n.github.io/linear-classify/#softmax-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train\n",
    "\n",
    "* https://github.com/pytorch/examples/blob/master/imagenet/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Hyperparameters**\n",
    "\n",
    "* WeightInitialization = HeUniform\n",
    "* Optimizer = RMSProp\n",
    "* LR = .001 with exponential decay of 0.995 after each epoch\n",
    "* Data Augmentation = Random Crops, Vertical Flips\n",
    "* ValidationSet with early stopping based on IoU or MeanAccuracy with patience of 100 (50 during finetuning)\n",
    "* WeightDecay = .0001\n",
    "* Finetune with full-size images, LR = .0001\n",
    "* Dropout = 0.2\n",
    "* BatchNorm \"we use current batch stats at training, validation, and test time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_predictions(output_batch):\n",
    "    # Variables(Tensors) of size (bs,12,224,224)\n",
    "    bs,c,h,w = output_batch.size()\n",
    "    tensor = output_batch.data\n",
    "    # Argmax along channel axis (softmax probabilities)\n",
    "    values, indices = tensor.cpu().max(1)\n",
    "    indices = indices.view(bs,h,w)\n",
    "    return indices\n",
    "\n",
    "def error(preds, targets):\n",
    "    assert preds.size() == targets.size()\n",
    "    bs,h,w = preds.size()\n",
    "    n_pixels = bs*h*w\n",
    "    incorrect = preds.ne(targets).cpu().sum()\n",
    "    err = 100.*incorrect/n_pixels\n",
    "    return round(err,5)\n",
    "        \n",
    "def train(model, trn_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    trn_loss = 0\n",
    "    trn_error = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trn_loader):\n",
    "        inputs, targets = Variable(inputs.cuda()), Variable(targets.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trn_loss += loss.data[0]\n",
    "        pred = get_predictions(output)\n",
    "        trn_error += error(pred, targets.data.cpu())\n",
    "    trn_loss /= len(trn_loader) #n_batches\n",
    "    trn_error /= len(trn_loader)\n",
    "    return trn_loss, trn_error\n",
    "\n",
    "def test(model, test_loader, criterion, epoch=1):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_error = 0\n",
    "    for data, target in test_loader:\n",
    "        data = Variable(data.cuda(), volatile=True)\n",
    "        target = Variable(target.cuda())\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        pred = get_predictions(output)\n",
    "        test_error += error(pred, target.data.cpu())\n",
    "    test_loss /= len(test_loader) #n_batches\n",
    "    test_error /= len(test_loader)\n",
    "    return test_loss, test_error\n",
    "\n",
    "def adjust_learning_rate(lr, decay, optimizer, cur_epoch, n_epochs):\n",
    "    \"\"\"Sets the learning rate to the initially \n",
    "        configured `lr` decayed by `decay` every `n_epochs`\"\"\"\n",
    "    new_lr = lr * (decay ** (cur_epoch // n_epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        #kaiming is first name of author whose last name is 'He' lol\n",
    "        init.kaiming_uniform(m.weight) \n",
    "        m.bias.data.zero_()\n",
    "\n",
    "def save_weights(model, epoch, loss, err, experiment, isBest=False):\n",
    "    weights_fname = experiment+'-%d-%.3f-%.3f.pth' % (epoch, loss, err)\n",
    "    weights_fpath = os.path.join(WEIGHTS_PATH, weights_fname)\n",
    "    torch.save({\n",
    "            'startEpoch': epoch+1,\n",
    "            'loss':loss,\n",
    "            'error': err,\n",
    "            'sessionName': experiment,\n",
    "            'state_dict': model.state_dict()\n",
    "        }, weights_fpath )\n",
    "    shutil.copyfile(weights_fpath, WEIGHTS_PATH+'latest.pth')\n",
    "    if isBest:\n",
    "        shutil.copyfile(weights_fpath, WEIGHTS_PATH+'best.pth')\n",
    "\n",
    "def load_weights(model, fpath):\n",
    "    print(\"loading weights '{}'\".format(fpath))\n",
    "    state = torch.load(fpath)\n",
    "    start_epoch = state['startEpoch']\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    print(\"loaded weights from session {} (lastEpoch {}, loss {}, error {})\"\n",
    "          .format(state['sessionName'], start_epoch-1, state['loss'],\n",
    "                  state['error']))\n",
    "    return state\n",
    "\n",
    "def save_optimizer(optimizer, epoch, experiment):\n",
    "    optim_fname = experiment+'-optim-%d.pth' % (epoch)\n",
    "    optim_fpath = os.path.join(WEIGHTS_PATH, optim_fname)\n",
    "    torch.save({\n",
    "            'lastEpoch': epoch,\n",
    "            'sessionName': experiment,\n",
    "            'state_dict': optimizer.state_dict()\n",
    "        }, optim_fpath )\n",
    "    shutil.copyfile(optim_fpath, WEIGHTS_PATH+'latest-optim.pth')\n",
    "\n",
    "def load_optimizer(optimizer, fpath):\n",
    "    print(\"loading optimizer '{}'\".format(fpath))\n",
    "    optim = torch.load(fpath)\n",
    "    optimizer.load_state_dict(optim['state_dict'])\n",
    "    print(\"loaded optimizer from session {}, lastEpoch {}\"\n",
    "          .format(optim['sessionName'], optim['lastEpoch']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Config\n",
    "\n",
    "* https://github.com/SimJeg/FC-DenseNet/blob/master/config/FC-DenseNet103.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 12 #11 + background\n",
    "seed = 0\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_DECAY = 0.995 # Applied each epoch \"exponential decay\"\n",
    "DECAY_LR_EVERY_N_EPOCHS = 1\n",
    "WEIGHT_DECAY = 0.0001\n",
    "N_EPOCHS = 1000\n",
    "MAX_PATIENCE = 50\n",
    "\n",
    "train_file = RESULTS_PATH+EXPERIMENT+'-train.csv'\n",
    "test_file = RESULTS_PATH+EXPERIMENT+'-test.csv'\n",
    "existing_weights_fpath=WEIGHTS_PATH+'latest.pth'\n",
    "existing_optimizer_fpath=WEIGHTS_PATH+'latest-optim.pth'\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "model = FCDenseNet103(n_classes=12).cuda()\n",
    "print('  + Number of params: {}'.format(\n",
    "    sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "state = load_weights(model, existing_weights_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if existing_weights_fpath:\n",
    "    state = load_weights(model, existing_weights_fpath)\n",
    "    start_epoch = state['startEpoch']\n",
    "    endEpoch = state['startEpoch'] + N_EPOCHS\n",
    "    print ('Resume training at epoch: {}'.format(state['startEpoch']))\n",
    "    if os.path.exists(train_file): #assume test.csv exists\n",
    "        append_write = 'a' # append if already exists\n",
    "    else:\n",
    "        append_write = 'w' # make a new file if not\n",
    "    trainF = open(os.path.join(train_file), append_write)\n",
    "    testF = open(os.path.join(test_file), append_write)\n",
    "else:\n",
    "    print (\"Training new model from scratch\")\n",
    "    model.apply(weights_init)\n",
    "    start_epoch = 1\n",
    "    endEpoch = N_EPOCHS\n",
    "    trainF = open(os.path.join(train_file), 'w')\n",
    "    testF = open(os.path.join(test_file), 'w')\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "if existing_optimizer_fpath:\n",
    "    print(\"Loading existing optimizer: \", existing_optimizer_fpath)\n",
    "    load_optimizer(optimizer, existing_optimizer_fpath)\n",
    "criterion = nn.NLLLoss2d(weight=camvid.class_weight.cuda()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_chart, err_chart, txt_chart = None, None, None\n",
    "best_loss_epoch = 0\n",
    "best_val_loss = sys.maxsize #very large number\n",
    "    \n",
    "for epoch in range(start_epoch, endEpoch+1):\n",
    "    since = time.time()\n",
    "\n",
    "    ### Train ###\n",
    "    trn_loss, trn_err = train(model, train_loader, optimizer, criterion, epoch)\n",
    "    print('Epoch {:d}: Train - Loss: {:.4f}\\tErr: {:.4f}'.format(epoch, trn_loss, trn_err))    \n",
    "    time_elapsed = time.time() - since  \n",
    "    print('Train Time {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    \n",
    "    ### Test ###\n",
    "    tst_loss, tst_err = test(model, val_loader, criterion, epoch)    \n",
    "    print('Test - Loss: {:.4f}, Error: {:.4f}'.format(tst_loss, tst_err))\n",
    "    time_elapsed = time.time() - since  \n",
    "    print('Total Time {:.0f}m {:.0f}s\\n'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    \n",
    "    ### Checkpoint ###\n",
    "    is_best_loss = tst_loss < best_val_loss\n",
    "    save_weights(model, epoch, tst_loss, tst_err, EXPERIMENT, is_best_loss)\n",
    "    save_optimizer(optimizer, epoch, EXPERIMENT)\n",
    "    \n",
    "    \n",
    "    ### Save Metrics ###\n",
    "    if trainF:\n",
    "        trainF.write('{},{},{}\\n'.format(epoch, trn_loss, trn_err))\n",
    "        trainF.flush()\n",
    "    if testF:\n",
    "        testF.write('{},{},{}\\n'.format(int(epoch), tst_loss, tst_err))\n",
    "        testF.flush()\n",
    "    \n",
    "    \n",
    "    ### Plot Online ###\n",
    "    if visdom_enabled:\n",
    "        loss_chart = viz_plot_tst_trn(loss_chart, epoch, \n",
    "                                      tst_loss, trn_loss, \n",
    "                                      'loss', EXPERIMENT)\n",
    "        err_chart = viz_plot_tst_trn(err_chart, epoch, \n",
    "                                     tst_err, trn_err, \n",
    "                                     'error', EXPERIMENT)\n",
    "        txt_chart = viz_plot_summary(txt_chart, epoch, \n",
    "                                     tst_loss, trn_loss, \n",
    "                                     tst_err, trn_err, \n",
    "                                     EXPERIMENT)\n",
    "\n",
    "        \n",
    "    ### Early Stopping ###\n",
    "    if is_best_loss:\n",
    "        best_loss_epoch = epoch\n",
    "        best_val_loss = tst_loss\n",
    "    elif (epoch-best_loss_epoch) > MAX_PATIENCE:\n",
    "        print((\"Early stopping at epoch %d since no \" \n",
    "               + \"better loss found since epoch %.3\") \n",
    "               % (epoch, best_val_loss))\n",
    "        break\n",
    "\n",
    "\n",
    "    ### Adjust Lr ###\n",
    "    adjust_learning_rate(LEARNING_RATE, LR_DECAY, optimizer, \n",
    "                         epoch, DECAY_LR_EVERY_N_EPOCHS)\n",
    "\n",
    "\n",
    "trainF.close()\n",
    "testF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss2d(weight=camvid.class_weight.cuda()).cuda()\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict(model, input_loader, n_batches=1):\n",
    "    input_loader.batch_size = 1\n",
    "    #Takes input_loader and returns array of prediction tensors\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    for input, target in input_loader:\n",
    "        data, label = Variable(input.cuda(), volatile=True), Variable(target.cuda())\n",
    "        output = model(data)\n",
    "        pred = get_predictions(output)\n",
    "        predictions.append([input,target,pred])\n",
    "    return predictions\n",
    "\n",
    "#predictions = predict_all(model, test_loader, 1)\n",
    "# for out in predictions[:1]:\n",
    "#     view_image(out[0][0])\n",
    "#     view_annotated(out[1][0])\n",
    "#     view_annotated(out[2][0])\n",
    "\n",
    "def view_sample_predictions(n):\n",
    "    #torch.cuda.manual_seed(random.randint(0,10**7))\n",
    "    test_loader = torch.utils.data.DataLoader(test_dset, batch_size=n, shuffle=True)\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    data, label = Variable(inputs.cuda(), volatile=True), Variable(targets.cuda())\n",
    "    output = model(data)\n",
    "    pred = get_predictions(output)\n",
    "    batch_size = inputs.size(0)\n",
    "    for i in range(batch_size):\n",
    "        view_image(inputs[i])\n",
    "        view_annotated(targets[i])\n",
    "        view_annotated(pred[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "view_sample_predictions(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "* https://github.com/mattmacy/vnet.pytorch/blob/master/train.py\n",
    "* https://github.com/SimJeg/FC-DenseNet/blob/master/FC-DenseNet.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
